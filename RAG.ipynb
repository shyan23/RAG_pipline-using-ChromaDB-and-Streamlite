{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyBu0XqCELY8T8XwUfnY6deQziTPHp8DE0I\n"
     ]
    }
   ],
   "source": [
    "gemini_api_key = \"AIzaSyBu0XqCELY8T8XwUfnY6deQziTPHp8DE0I\"\n",
    "import os\n",
    "os.environ[\"GEMINI_API_KEY\"]=gemini_api_key\n",
    "\n",
    "val = os.environ.get(\"GEMINI_API_KEY\")\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Reads the text content from a PDF file and returns it as a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The file path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    - str: The concatenated text content of all pages in the PDF.\n",
    "    \"\"\"\n",
    "    # Logic to read pdf\n",
    "    reader = PdfReader(file_path)\n",
    "\n",
    "    # Loop over each page and store it in a variable\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "# replace the path with your file path\n",
    "pdf_text = load_pdf(file_path=\"state_of_the_union.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def split_text(text: str):\n",
    "    \"\"\"\n",
    "    Splits a text string into a list of non-empty substrings based on the specified pattern.\n",
    "    The \"\\n \\n\" pattern will split the document para by para\n",
    "    Parameters:\n",
    "    - text (str): The input text to be split.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list containing non-empty substrings obtained by splitting the input text.\n",
    "\n",
    "    \"\"\"\n",
    "    split_text = re.split('\\n \\n', text)\n",
    "    return [i for i in split_text if i != \"\"]\n",
    "\n",
    "chunked_text = split_text(text=pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "import os\n",
    "\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    \"\"\"\n",
    "    Custom embedding function using the Gemini AI API for document retrieval.\n",
    "\n",
    "    This class extends the EmbeddingFunction class and implements the __call__ method\n",
    "    to generate embeddings for a given set of documents using the Gemini AI API.\n",
    "\n",
    "    Parameters:\n",
    "    - input (Documents): A collection of documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - Embeddings: Embeddings generated for the input documents.\n",
    "    \"\"\"\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "        genai.configure(api_key=gemini_api_key)\n",
    "        model = \"models/embedding-001\"\n",
    "        title = \"Custom query\"\n",
    "        return genai.embed_content(model=model,\n",
    "                                   content=input,\n",
    "                                   task_type=\"retrieval_document\",\n",
    "                                   title=title)[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from typing import List\n",
    "def create_chroma_db(documents:List, path:str, name:str):\n",
    "    \"\"\"\n",
    "    Creates a Chroma database using the provided documents, path, and collection name.\n",
    "\n",
    "    Parameters:\n",
    "    - documents: An iterable of documents to be added to the Chroma database.\n",
    "    - path (str): The path where the Chroma database will be stored.\n",
    "    - name (str): The name of the collection within the Chroma database.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[chromadb.Collection, str]: A tuple containing the created Chroma Collection and its name.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    db = chroma_client.create_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "\n",
    "    for i, d in enumerate(documents):\n",
    "        db.add(documents=d, ids=str(i))\n",
    "\n",
    "    return db, name\n",
    "\n",
    "db,name =create_chroma_db(documents=chunked_text, \n",
    "                          path=\"/home/shyan/Desktop/RAG_PIPELINE/DBMS\", #replace with your path\n",
    "                          name=\"rag_experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chroma_collection(path, name):\n",
    "    \"\"\"\n",
    "    Loads an existing Chroma collection from the specified path with the given name.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The path where the Chroma database is stored.\n",
    "    - name (str): The name of the collection within the Chroma database.\n",
    "\n",
    "    Returns:\n",
    "    - chromadb.Collection: The loaded Chroma Collection.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    db = chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "\n",
    "    return db\n",
    "\n",
    "db=load_chroma_collection(path=\"/home/shyan/Desktop/RAG_PIPELINE/DBMS\", name=\"rag_experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_passage(query, db, n_results):\n",
    "  passage = db.query(query_texts=[query], n_results=n_results)['documents'][0]\n",
    "  return passage\n",
    "\n",
    "#Example usage\n",
    "relevant_text = get_relevant_passage(query=\"Sanctions on Russia\",db=db,n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rag_prompt(query, relevant_passage):\n",
    "  escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "  prompt = (\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below. \\\n",
    "  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \\\n",
    "  However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \\\n",
    "  strike a friendly and converstional tone. \\\n",
    "  If the passage is irrelevant to the answer, you may ignore it.\n",
    "  QUESTION: '{query}'\n",
    "  PASSAGE: '{relevant_passage}'\n",
    "\n",
    "  ANSWER:\n",
    "  \"\"\").format(query=query, relevant_passage=escaped)\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "def Generate_answer(prompt):\n",
    "    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not gemini_api_key:\n",
    "        raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    answer = model.generate_content(prompt)\n",
    "    return answer.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Together with allies, powerful economic sanctions have been enforced against Russia.  The largest Russian banks have been cut off from the international financial system, with the Russian central bank blocked from defending the Ruble, thus making Putin's \"$630 Billion \"war fund\" worthless. Russia's access to technology that would strengthen its economy and military has also been restricted for years to come.  The US has also joined its allies in closing off airspace for all Russian flights, further isolating the country and putting additional pressure on its economy.  Due to these actions, the Ruble has lost 30% of it value, with the Russian stock market dropping 40% and trading is still suspended.  The Russian economy is weakened due to these sanctions.  In addition to these sanctions, the free world is working to hold him accountable for his actions, with 27 members of the European Union, along with other countries such as the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and even Switzerland, inflicting pain on Russia and supporting Ukraine.  As a result, Putin is more isolated from the world than ever before.\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(db,query):\n",
    "    #retrieve top 3 relevant text chunks\n",
    "    relevant_text = get_relevant_passage(query,db,n_results=3)\n",
    "    prompt = make_rag_prompt(query, \n",
    "                             relevant_passage=\"\".join(relevant_text)) # joining the relevant chunks to create a single passage\n",
    "    answer = Generate_answer(prompt)\n",
    "\n",
    "    return answer\n",
    "\n",
    "db=load_chroma_collection(path=\"/home/shyan/Desktop/RAG_PIPELINE/DBMS\", #replace with path of your persistent directory\n",
    "                          name=\"rag_experiment\") #replace with the collection name\n",
    "\n",
    "answer = generate_answer(db,query=\"what sanctions have been placed on Russia\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'query'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m db\u001b[38;5;241m=\u001b[39mload_chroma_collection(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/shyan/Desktop/RAG_PIPELINE/DBMS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#replace with path of your persistent directory\u001b[39;00m\n\u001b[1;32m      2\u001b[0m                           name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_experiment\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#replace with the collection name\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat sanctions have been placed on Russia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "Cell \u001b[0;32mIn[72], line 6\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(db, query)\u001b[0m\n\u001b[1;32m      3\u001b[0m relevant_text \u001b[38;5;241m=\u001b[39m get_relevant_passage(query,db,n_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m make_rag_prompt(query, \n\u001b[1;32m      5\u001b[0m                          relevant_passage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(relevant_text)) \u001b[38;5;66;03m# joining the relevant chunks to create a single passage\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "Cell \u001b[0;32mIn[72], line 3\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(db, query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_answer\u001b[39m(db,query):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#retrieve top 3 relevant text chunks\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     relevant_text \u001b[38;5;241m=\u001b[39m \u001b[43mget_relevant_passage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m make_rag_prompt(query, \n\u001b[1;32m      5\u001b[0m                              relevant_passage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(relevant_text)) \u001b[38;5;66;03m# joining the relevant chunks to create a single passage\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     answer \u001b[38;5;241m=\u001b[39m generate_answer(prompt,query)\n",
      "Cell \u001b[0;32mIn[57], line 2\u001b[0m, in \u001b[0;36mget_relevant_passage\u001b[0;34m(query, db, n_results)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_relevant_passage\u001b[39m(query, db, n_results):\n\u001b[0;32m----> 2\u001b[0m   passage \u001b[38;5;241m=\u001b[39m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m(query_texts\u001b[38;5;241m=\u001b[39m[query], n_results\u001b[38;5;241m=\u001b[39mn_results)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m passage\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'query'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
